from fastapi import FastAPI
from pydantic import BaseModel
import google.generativeai as genai
import google.api_core.exceptions
import uvicorn
from fastapi.middleware.cors import CORSMiddleware
import os

# Configure Gemini API key securely
genai.configure(api_key="AIzaSyDm1y_Pjd_SGWoZ0kkyVta8tcsnPjFFCiE")

# Initialize FastAPI app
app = FastAPI(title="Role-based Logical Ability AI Agent")

# Allow CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request model
class LogicRequest(BaseModel):
    role: str
    experience_level: str
    target_company: str

# Response model
class LogicResponse(BaseModel):
    question: str
    tip: str = "This logical reasoning question is generated by AI. Focus on clarity and step-by-step problem solving."

# Prompt template
LOGIC_PROMPT_TEMPLATE = (
    "You are preparing a logical reasoning question for a {experience_level} candidate "
    "applying for a {role} position at {target_company}. "
    "The question should assess logical thinking, pattern recognition, or analytical reasoning. "
    "Avoid overly technical or domain-specific content.\n\n"
    "Logical Reasoning Question:"
)

# Endpoint
@app.post("/logical-ability", response_model=LogicResponse)
async def generate_role_based_logic_question(req: LogicRequest):
    prompt = LOGIC_PROMPT_TEMPLATE.format(
        role=req.role.strip(),
        experience_level=req.experience_level.strip(),
        target_company=req.target_company.strip()
    )

    try:
        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(prompt)
        question_text = response.text.strip()
    except google.api_core.exceptions.GoogleAPIError:
        question_text = "Sorry, I'm unable to generate a logical ability question right now."

    return LogicResponse(question=question_text)

# Run server
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=10000)
